import os #operacje na plikach, folderach, ≈õcie≈ºkach itp
from absl import logging
import warnings
import math
import cv2
import random
from collections import deque
import mediapipe as mp  # [MP] biblioteka do analizy d≈Çoni
print("MediaPipe dzia≈Ça poprawnie!")

# ============================================================
#  KONFIGURACJA
# ============================================================

# Wy≈ÇƒÖczenie log√≥w TensorFlow i MediaPipe
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
os.environ["GLOG_minloglevel"] = "3"
logging.set_verbosity(logging.ERROR)
warnings.filterwarnings("ignore", message=".*Feedback manager*")

# [MP] Inicjalizacja MediaPipe Hands
mp_hands = mp.solutions.hands #pobiera modu≈Ç hands z pakietu mediapipe do ykrywania i ≈õledzenia d≈Çoni
mp_drawing = mp.solutions.drawing_utils #pobiera modu≈Ç drawing_utils z mediapipe do rysowania landmark√≥w i po≈ÇƒÖcze≈Ñ d≈Çoni na obrazie (linie i punkty)

# ============================================================
#  FUNKCJE POMOCNICZE
# ============================================================

def ekstraktuj_punkty(hand_landmarks):
    """Zwraca znormalizowane punkty d≈Çoni (x, y) wzglƒôdem nadgarstka."""
    punkty = [(lm.x, lm.y) for lm in hand_landmarks.landmark] #Pobiera wsp√≥≈Çrzƒôdne wszystkich punkt√≥w d≈Çoni
    ref_x, ref_y = punkty[0]
    punkty_shifted = [(x - ref_x, y - ref_y) for x, y in punkty] #Przesuwa uk≈Çad odniesienia tak, ≈ºeby punkt 0 (nadgarstek) by≈Ç w (0, 0)

    base_len = math.dist(punkty[0], punkty[9]) or 1.0 #Normalizuje rozmiar d≈Çoni ‚Äî dzieli przez odleg≈Ço≈õƒá miƒôdzy punktem 0 (nadgarstek) a 9 (≈õrodek d≈Çoni).
    return [(x / base_len, y / base_len) for x, y in punkty_shifted] #Zwraca listƒô punkt√≥w (x, y) znormalizowanych ‚Äî czyli niezale≈ºnych od odleg≈Ço≈õci i po≈Ço≈ºenia.


def porownaj_szkielety(szk1, szk2):
    """Por√≥wnuje dwa szkielety d≈Çoni ‚Äî im mniejszy dystans, tym wiƒôksze podobie≈Ñstwo."""
    if len(szk1) != len(szk2):
        return float("inf")
    return sum(math.dist(a, b) for a, b in zip(szk1, szk2)) / len(szk1)
#Liczy ≈õredniƒÖ odleg≈Ço≈õƒá miƒôdzy odpowiadajƒÖcymi sobie punktami, im mniejsza warto≈õƒá tym bardziej podobne gesty

# ============================================================
#  WCZYTYWANIE WZORC√ìW STATYCZNYCH
# ============================================================

def wczytaj_wzorce(folder_path):
    """Wczytuje obrazy wzorc√≥w gest√≥w statycznych z folderu."""
    print(f"üìÇ Wczytywanie wzorc√≥w z: {folder_path}")
    wzorce = {}

    with mp_hands.Hands(static_image_mode=True, max_num_hands=1) as hands: #Je≈õli znajdzie d≈Ço≈Ñ
        for filename in os.listdir(folder_path):
            path = os.path.join(folder_path, filename)
            frame = cv2.imread(path)
            if frame is None:
                print(f"‚ö† Nie mo≈ºna wczytaƒá pliku: {filename}")
                continue

            results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            if not results.multi_hand_landmarks:
                print(f"‚ö† Brak d≈Çoni w {filename}")
                continue

            label = os.path.splitext(filename)[0].lower() #etykieta gestu to nazwa pliku bez rozszerzenia
            wzorce[label] = ekstraktuj_punkty(results.multi_hand_landmarks[0])  #zapisuje jej znormalizowany szkielet
            print(f"‚úÖ Za≈Çadowano: {label}")

    print(f"üìÅ ≈ÅƒÖcznie {len(wzorce)} wzorc√≥w wczytanych.\n")
    return wzorce

# ============================================================
#  WCZYTYWANIE WZORC√ìW DYNAMICZNYCH
# ============================================================

def wczytaj_wzorce_dynamiczne(folder_path):
    """
    Wczytuje wzorce gest√≥w dynamicznych z plik√≥w wideo.
    Ka≈ºdy wzorzec jest listƒÖ klatek, a ka≈ºda klatka to lista punkt√≥w d≈Çoni.
    """
    print(f"üé• Wczytywanie wzorc√≥w dynamicznych z: {folder_path}")
    wzorce_dyn = {}

    with mp_hands.Hands(
        static_image_mode=False,
        max_num_hands=1,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5
    ) as hands:

        for filename in os.listdir(folder_path):
            if not filename.lower().endswith((".mp4", ".avi", ".mov")):
                continue

            path = os.path.join(folder_path, filename)
            cap = cv2.VideoCapture(path)
            if not cap.isOpened():
                print(f"‚ö† Nie mo≈ºna otworzyƒá pliku: {filename}")
                continue

            sekwencja = []
            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = hands.process(image_rgb)
                if results.multi_hand_landmarks:
                    hand_landmarks = results.multi_hand_landmarks[0]
                    punkty = ekstraktuj_punkty(hand_landmarks)
                    sekwencja.append(punkty)

            cap.release()
            label = os.path.splitext(filename)[0].lower()
            if sekwencja:
                wzorce_dyn[label] = sekwencja
                print(f"‚úÖ Za≈Çadowano gest dynamiczny: {label} ({len(sekwencja)} klatek)")
            else:
                print(f"‚ö† Brak d≈Çoni w wideo: {filename}")

    print(f"üìÅ ≈ÅƒÖcznie {len(wzorce_dyn)} wzorc√≥w dynamicznych.\n")
    return wzorce_dyn


# Funkcja "za≈õlepka" ‚Äì klasyfikacja statyczna
#with to konstrukcja w Pythonie u≈ºywana do zarzƒÖdzania kontekstem.
# Oznacza to, ≈ºe automatycznie wykonuje pewne czynno≈õci przy wej≈õciu i wyj≈õciu z bloku kodu.
def klasyfikuj_stat(frame, wzorce_stat):
    max_allowed_distance = 0.2 # spr√≥bowac dobraƒá
    # [MP] Utw√≥rz obiekt Hands dla pojedynczej klatki (statyczny gest)
    with mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.3) as hands:
        # [MP] Konwersja obrazu na RGB dla mediapipe zamiast BGR
        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = hands.process(image_rgb)

        # [MP] Je≈õli wykryto d≈Ço≈Ñ ‚Äì narysuj szkielet
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)
				# --- Klasyfikacja statyczna ---
                szkiel_test = ekstraktuj_punkty(hand_landmarks) #Oblicza aktualny szkielet
                najlepszy, min_dist = "-", float("inf")

                for litera, szkiel_wzorzec in wzorce_stat.items():
                    dist = porownaj_szkielety(szkiel_test, szkiel_wzorzec) #Por√≥wnuje go z ka≈ºdym wzorcem
                    if dist < min_dist:
                        najlepszy, min_dist = litera, dist
                if min_dist > max_allowed_distance:
                    return '-'
                return najlepszy

    return '-'

# Funkcja "za≈õlepka" ‚Äì klasyfikacja dynamiczna
def klasyfikuj_dyn(buffer, wzorce_dyn):
    """
    Klasyfikuje ruch d≈Çoni jako gest dynamiczny,
    por√≥wnujƒÖc przebieg klatek z wzorcami dynamicznymi.
    """
    # Utworzenie szkielet√≥w z bufora bie≈ºƒÖcego
    with mp_hands.Hands(static_image_mode=False, max_num_hands=1) as hands:
        test_seq = []
        for frame in buffer:
            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = hands.process(image_rgb)
            if results.multi_hand_landmarks:
                hand_landmarks = results.multi_hand_landmarks[0]
                test_seq.append(ekstraktuj_punkty(hand_landmarks))

    if not test_seq:
        return '-'

    # Proste dopasowanie sekwencji z karƒÖ za r√≥≈ºnicƒô d≈Çugo≈õci
    def porownaj_sekwencje(seq1, seq2):
        min_len = min(len(seq1), len(seq2))
        dystanse = [porownaj_szkielety(seq1[i], seq2[i]) for i in range(min_len)]
        sredni = sum(dystanse) / len(dystanse)
        kara_dlugosc = abs(len(seq1) - len(seq2)) * 0.02
        return sredni + kara_dlugosc

    najlepszy, min_dist = '-', float('inf')
    for gest, seq_wzorzec in wzorce_dyn.items():
        dist = porownaj_sekwencje(test_seq, seq_wzorzec)
        if dist < min_dist:
            min_dist = dist
            najlepszy = gest

    # Pr√≥g dopasowania (dobierany eksperymentalnie)
    if min_dist > 0.3:
        return '-'
    return najlepszy


def przetworz_video(video_path, wzorce_stat, wzorce_dyn):
    print(f"\n‚ñ∂Ô∏è Rozpoczynam analizƒô pliku: {video_path}")
    bufor = deque(maxlen=25)
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        print(f"‚ùå Nie mo≈ºna otworzyƒá pliku wideo: {video_path}")
        return

    # gesty, kt√≥re mogƒÖ byƒá poczƒÖtkiem dynamicznych
    potencjalnie_dynamiczne = {'a','c','e','i','l','n','o','r','s'}

    while True:
        ret, frame = cap.read()
        if not ret:
            print(f"üé¨ Koniec filmu: {video_path}")
            break

        bufor.append(frame)
        if len(bufor) < bufor.maxlen:
            cv2.imshow("PodglƒÖd", frame)
            if cv2.waitKey(30) & 0xFF == ord('q'):
                break
            continue

        wynik_stat = klasyfikuj_stat(bufor[0], wzorce_stat)
        if wynik_stat == '-':
            cv2.imshow("PodglƒÖd", frame)
            if cv2.waitKey(30) & 0xFF == ord('q'):
                break
            continue

        if wynik_stat in potencjalnie_dynamiczne:
            wynik_dyn = klasyfikuj_dyn(list(bufor), wzorce_dyn)
            if wynik_dyn != '-':
                print(f"üí´ Wykryto gest dynamiczny: {wynik_dyn}")
                bufor.clear()
            else:
                print(f"üëå Wykryto gest statyczny: {wynik_stat}")
        else:
            print(f"üëå Wykryto gest statyczny: {wynik_stat}")

        cv2.imshow("PodglƒÖd", frame)
        if cv2.waitKey(30) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

# ============================================================
#  MAIN
# ============================================================

def main():
    videos = ["pfa2.mp4"]

    wzorce_stat = wczytaj_wzorce(r"C:\\Users\\Ola Sawicka\\Desktop\\semestr 7\\thesis\\statycze")
    wzorce_dyn = wczytaj_wzorce_dynamiczne(r"C:\\Users\\Ola Sawicka\\Desktop\\semestr 7\\thesis\\dynamiczne")

    for video in videos:
        przetworz_video(video, wzorce_stat, wzorce_dyn)

    print("\n‚úÖ Wszystkie pliki zosta≈Çy przetworzone!")


if __name__ == "__main__": #uruchomi≈Ç main() tylko wtedy, gdy ten plik zosta≈Ç uruchomiony bezpo≈õrednio, a nie np. zaimportowany z innego pliku.
    main()